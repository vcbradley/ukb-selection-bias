---
title: "Methods for Selection Bias in the UK Biobank"
author: "Valerie Bradley"
date: "15 October 2019"
output: 
  beamer_presentation:
    theme: "Madrid"
    slide_level: 2
    includes:
      in_header: cust_formatting.tex
    keep_tex: yes
    toc: true
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(ggplot2)
library(data.table)
library(knitr)
library(kableExtra)
library(gridExtra)
library(grid)
library(lattice)
library(ggpubr)


# plotting function
getDistPlot = function(data, title = NULL, weighted = F){
  
  plot = ggplot() +
  geom_bar(data = data[variable %in% c('samp_prop', 'ukb_prop'),]
           , aes(x = level, y = as.numeric(as.character(value)), fill = variable), position = 'dodge', stat = 'identity') +
  # geom_bar(data = data[variable == ,]
  #          , aes(x = level, y = as.numeric(as.character(value)), color = 'UK pop'), alpha = 0, stat = 'identity') +
      geom_bar(data = data[variable == 'pop_prop_weighted', ]
           , aes(x = level, y = as.numeric(as.character(value)), color = 'population'), alpha = 0, stat = 'identity') +
  facet_wrap(~var_display, nrow = 2, ncol = 2, scales = 'free_x') + 
  theme_minimal() +
  scale_color_manual(values = c('black'), name = c(""), labels = c("UK pop")) +
  scale_fill_manual(values = c('#9bcbeb', '#418FDE'), name = c("UK Biobank"), labels = c("full", "imaging")) +
  #ggtitle(paste0("Distribution comparison - ", var)) +
    ylab('') +
    xlab('variable') +
  scale_y_continuous(labels=scales::percent) +
    theme(axis.text.x = element_text(angle = 30, hjust = 1), plot.title = element_text(size=22))
  
  if(is.null(title)){
    plot = plot + ggtitle("Distributions of variables")
  }else{
    plot = plot + ggtitle(title)
  }
  
  return(plot)
}

load('~/github/mini-project-1/weighting/ukb_weighted_hse16_summary.rda')
# pop_prop = HSE, unweighted
# pop_prop_weighted = HSE, weighted

# ADD IN APOE PREVALENCE
weight_summary[var == '16-ApoE Phenotype', var := '16-ApoE Genotype']


```



# Unrepresentativeness in the UK Biobank

## About the data

__UK Biobank__

* __Goal__: Study exposures and outcomes that affect aging populations
* 500,000+ participants aged 40-70 when recruited between 2006 and 2010
* _Not representative of the UK general population_ [@fry2017comparison]

__UK Biobank imaging cohort__

* Subset of UKB participants, undergo additional imaging exams
* 21,407 complete, valid T1 structural MRIs (4% of UKB)

__[2016 Health Survey for England](https://digital.nhs.uk/data-and-information/publications/statistical/health-survey-for-england/health-survey-for-england-2016)__

* Designed to estimate prevalence of health outcomes (e.g. smoking, obesity, high blood pressure)
* 2016 survey contains 10,067 respondents, 4,318 aged 45-80



## Quantifying unrepresentativeness of the UKB

```{r plot-selection-bias-demos, warning=FALSE, message=FALSE, echo=F, out.width='95%', fig.align='center'}
# head(weight_summary)
# weight_summary[, unique(var)]
# melt(weight_summary[, .(var, level, pop_prop_weighted, samp_prop)],id.vars = c('var', 'level'))
var_list = c('03-Age Bucket', '05-Ethnicity', '15-Home ownership', '10-Occupation')

plot_data = melt(weight_summary[var %in% var_list, .(var, level, pop_prop_weighted, ukb_prop, samp_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]

getDistPlot(plot_data, title = "Demographic covariates")

```



## Quantifying unrepresentativeness of the UKB

```{r plot-selection-bias-health, warning=FALSE, message=FALSE, echo=F, out.width = '95%', fig.align='center'}

var_list = c('20-Smoking Status', '21-BMI Bucket', '23-Diabetes Ever')

plot_data = melt(weight_summary[var %in% var_list, .(var, level, pop_prop_weighted, ukb_prop, samp_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]

getDistPlot(data = plot_data, title = "Health covariates")
```

# Why is unrepresentativeness a problem?

## Quick note on structural causal models (SCMs)

SCMs use directed acyclic graphs (DAGs) as ``a mathematical language for integrating statistical and subject-matter information," specifically information about dependence structures [@Pearl1995]

* __nodes__ represent *physical mechanisms*
* __edges__ represent *direct causal pathways*

\begin{figure}
\centering
\begin{tikzpicture}
  	\node[state, minimum width=0.5cm] (w) at (-2,0) {W};
	\node[state, minimum width=0.5cm] (x) at (0,0) {X};
	\node[state, minimum width=0.5cm] (y) at (2,0) {Y};
	\node[state, minimum width=0.5cm] (z) at (0,1) {Z};
	
		\path (x) edge (y);
	\path (w) edge (x);
	\path (z) edge (y);	
\end{tikzpicture}
\end{figure}

Things we can say:

* there is a _direct_ causal pathway from $X$ to $Y$
* there is an _indirect_ causal pathway from $W$ to $Y$
* $X$ _d-separates_ $W$ and $Y$, such that $W \ci Y | X$


## SCM Example

\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES};
	\node[state, double] (s) at (2.5,0) {Selection};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol.};
	\node[state] (z) at (5,1.25) {Dementia};

	\path (x) edge[green] (s);
	\path (y) edge (z);
	\path (z) edge[red] (s);
	%\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

* SES and participation in UKB are \textcolor{green}{positively} correlated
* Dementia and participation are \textcolor{red}{negatively} correlated
* No causal relationship between SES and hippocampal volume (no edge)

Are SES and Dementia independent?

## Collider bias

```{r fig-2, out.width='40%'}
# include_graphics('~/github/mini-project-1/presentation/selection-bias-ex'
#                  , auto_pdf = getOption("knitr.graphics.auto_pdf", FALSE)
#                  , dpi = NULL)
```
\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES (E)};
	\node[state, double] (s) at (2.5,0) {Selection (S)};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol. (H)};
	\node[state] (z) at (5,1.25) {Dementia (D)};

	\path (x) edge (s);
	\path (y) edge (z);
	\path (z) edge (s);
	\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

**Collider bias**: "When two variables independently influence a third variable, and that third variable is conditioned upon" [@Munafo2018]

Opens a *backdoor path* (spurious association) between E and H.

* Want to know \(\text{P(H|E)}\)
* But only observe \(\text{P}(H | E, S = 1)\)
* And, because $H$, $E$ are *not* independent of $S$, those are not the same

## Collider bias example

@Day2016: GWAS using 142,630 observations from the UKB, induced strong collider bias

\begin{figure}
\centering
\includegraphics[width=\textwidth]{collider_bias_example.png}
\end{figure}


# Can we recover from selection bias?


## Formal conditions for recovery

Consider the *association* between \(X\) and \(Y\), \(\text{P}(y|x)\)

We can recover from selection bias **if and only if** \(\text{P}(y|x)\) can be written in terms of the quantities observed under selection; use *auxilary variables* \(\mathbf{Z}\)

* if \(Y \ci S | \{\mathbf{Z}, X\}\), then
$$\text{P}(y|x) = \sum_\mathbf{z} \text{P}(y|x, \mathbf{z}, S=1)\text{P}(\mathbf{z}|x)$$
* If \(\{Y \cup X\} \ci S | \mathbf{Z}\), then
$$\text{P}(y|x) = \sum_\mathbf{z} \text{P}(y|x, \mathbf{z}, S=1)\text{P}(\mathbf{z})$$

**The key:** conditioning on \(\mathbf{Z}\) (and maybe \(X\)) makes \(Y\) *conditionally independent of selection* \(S\)

$\text{P}(\mathbf{z})$ and $\text{P}(\mathbf{z}|x)$ are population distributions

## Example

Back to our example...

\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES (E)};
	\node[state, double] (s) at (2.5,0) {Selection (S)};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol. (H)};
	\node[state] (z) at (5,1.25) {Dementia (D)};

	\path (x) edge (s);
	\path (y) edge (z);
	\path (z) edge (s);
	\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

\begin{itemize}
\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \textcolor{red}{\ding{56}}}}$\square$] \(\{H \cup E\} \ci S | D\)
\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \textcolor{green}{\ding{52}}}}$\square$] \(H \ci S | D, E\)
\end{itemize}

So, can only recover \(\text{P}(H | E)\) as
$$\text{P}(H | E) = \sum_D\text{P}(H | E, D, S = 1) P (D|E)$$
*as long as we observe* \(\text{P}(D | E)\)

## Problems

1. Must assume we have correctly represented the dependence structures (and have correctly identified all necessary elements of $\mathbf{Z}$).  This is __hard__ to do in practice.

2. Don't always observe \(\text{P}(\mathbf{Z} | X)\) or \(\text{P}(\mathbf{Z})\) in full.  *Can we estimate it?*

3. Can only condition on a limited number of discrete variables $\mathbf{Z}$ before this breaks down.  *What happens when an element of $\mathbf{Z}$ is continuous?*


# Methods for recovering from selection bias

## Two main methods for recovery
Classes of methods for estimating effects, associations, prevalence in the presence of selection bias:

* **Inverse probability weighting (IPW)**: weight each observed unit by the inverse of the probability of selection, $w_i \propto 1/\text{P}(S = 1 | \mathbf{Z}_i)$
  * \textcolor{oxfordgreen}{pro}: weights are independent of $Y$ and $X$
  * \textcolor{oxfordred}{con}: not always clear how to incorporate into estimators
  * \textcolor{oxfordred}{con}: hard to correctly estimate standard errors of weighted estimators
\vspace{0.25cm}
* **Regression methods**: directly model the outcome of interest $\text{P}(Y|X)$, accounting for confounders/auxiliary variables 
  * \textcolor{oxfordred}{con}: have to estimate separate model for each outcome



## Inverse probability weighting (IPW)

Can be derived exactly from (causal) recovery conditions [@Correa2018]:
\begin{align}
\text{P}(\mathbf{y}|do(\mathbf{x})) &= \sum_\mathbf{Z} \text{P}(\mathbf{y}|\mathbf{x}, \mathbf{z}, S=1)\text{P}(\mathbf{z}\setminus\mathbf{z^T}|\mathbf{z^T}, S = 1)\text{P}(\mathbf{z^T})\\
&= \sum_\mathbf{Z} \frac{\textcolor{oxfordred}{\text{P}(\mathbf{y}, \mathbf{x}, \mathbf{z} | S = 1)}}{\textcolor{oxfordlightblue}{\text{P}(\mathbf{x}| \mathbf{z}, S = 1)}} \textcolor{oxfordgreen}{\frac{\text{P}(S = 1)}{\text{P}(S = 1 | \mathbf{z^T})}} \label{eq:IPSW}
\end{align}

* $\textcolor{oxfordred}{\text{P}(\mathbf{y}, \mathbf{x}, \mathbf{z} | S = 1)}$, joint distribution of $\mathbf{Y}$, $\mathbf{X}$ and $\mathbf{Z}$ under selection bias
* $\textcolor{oxfordlightblue}{\text{P}(\mathbf{x}| \mathbf{z}, S = 1)}$, the probability of treatment given covariates in the selection-biased sample data
    * related to the *propensity score*  $\text{P}(X|\mathbf{Z})$
* $\textcolor{oxfordgreen}{\text{P}(S = 1)/\text{P}(S = 1 | \mathbf{z^T})}$, the \textit{inverse probability-of-selection weight}

**But how should we estimate $\text{P}(S = 1 | \mathbf{Z})$?**


## Estimating $\text{P}(S = 1 | \mathbf{Z})$

Things to think about when comparing methods for estimating $\text{P}(S = 1 | \mathbf{Z})$:

1. Weighted **marginal distribution** of $\mathbf{Z}$ should match that of the population

2. Avoid extreme weights that inflate the **variance** of estimators (have to account for the additional uncertainty of weights)

3. **Computational complexity** of the method

4. How well does the method account for interactions between elements of $\mathbf{Z}$


## Methods: summary

Classic weighting methods:

1. *Post-stratification*: adjust to the joint distribution of \(\mathbf{Z}\) 
2. *Raking*: iteratively adjust the marginal distributions of elements in \(\mathbf{Z}\)
3. *Calibration*: raking, but with continuous variables as well as discrete variables

Less-common methods:

4. *Logit*: estimate the probability of selection directly
5. *LASSO*: use a LASSO to select variables and interactions for raking

New method:

6. *BART + raking*: use a Bayesian Additive Regression Tree (BART) to estimate the probability of selection, then rake such that key marginal distributions match those of the population



# Application to the UK Biobank

## Simulation overview

1. Generate a probability of missingness $p_i$ for all 21,407 imaging subjects in the UKB
    * $p_i$ depends on covariates that we know to be related to brain volume (mainly age) so that samples are biased
2. For sample sizes in $n_{sim} = 21,407 \times (0.01, 0.02, 0.04,0.05,0.075, 0.1, 0.25)$:
    1. Draw sample of size $n_{sim}$ from imaging cohort with probability proportional to $p_i$
    2. Weight sample to $P(\mathbf{Z})$ defined by UKB imaging cohort using each of 6 methods
    3. Perform steps 2.1-2.2 1000 times
    

## Simulation overview
    
Evaluate methods based on:

* **design effect**: measures the decrease in effective sample size (ESS) from weighting
$$\text{deff}(\mathbf{w}) = 1 + \text{Var}(\mathbf{w})$$
$$ESS = \frac{n}{\text{deff}(\mathbf{w})}$$

* **Absolute bias** of estimated average total brain volume

$$\text{bias} = |\bar{Y}^w - \mu|$$
$$\bar{Y}^w = \frac{1}{n_{sim}}\sum_{i = 1}^{n_{sim}} Y_i \hat{w}_i$$

## Simulation results

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{~/github/mini-project-1/simulation/results/sim_1_5000_old_v3/plots/sim_results_presentation.png}
\end{figure}

## Simulation results - Remarks

* _Post-stratification_ performs surprisingly well, likely due to the size of the data set
    * note that stratification variables chosen with a random forest (not the standard implementation)
* _BART + raking_ outperforms other methods at the smallest sample sizes (which is probably the most realistic setting)
* _Calibration_ performs very poorly - fails to correct bias and has large variance (likely from sensitivity to age distribution)
* Directly predicting $P(S = 1 | \mathbf{Z})$ with _logistic regression_ corrected bias well, but at a rather large cost in variance
* _LASSO_ has smallest deff, but also fails to correct bias (variable selection not working well)



## Application to UK Biobank imaging cohort
```{r fig-ukb-results-demo}
var_list = c('03-Age Bucket', '05-Ethnicity', '15-Home ownership', '10-Occupation')

plot_data = melt(weight_summary[var %in% var_list & !is.na(bart_prop) & !is.na(pop_prop_weighted), .(var, level, pop_prop_weighted, samp_prop, rake_prop, strat_prop, calib_prop, lasso_prop, logit_prop, bart_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]
plot_data[, method := gsub('_prop','', variable)]


ggplot() +
  geom_bar(data = plot_data[variable %in% c('samp_prop', 'bart_prop'),], aes(x = level, y = as.numeric(as.character(value)), fill = variable), position = 'dodge', stat = 'identity') +
  geom_bar(data = plot_data[variable %in% c('pop_prop_weighted'), ]
           , aes(x = level, y = as.numeric(as.character(value)), color = 'population'), alpha = 0, stat = 'identity') +
   facet_wrap(~var_display, nrow = 2, ncol = 2, scales = 'free_x') + 
  theme_minimal() +
  scale_color_manual(values = c('black', 'red'), name = c(""), labels = c("UK pop", 'sample')) +
  scale_fill_manual(values = c('#9bcbeb', '#418FDE'), name = c("UKB imaging"), labels = c("raw", "weighted")) +
  ggtitle("Weighted demographic distributions") +
    ylab('') +
    xlab('variable') +
  scale_y_continuous(labels=scales::percent) +
    theme(axis.text.x = element_text(angle = 30, hjust = 1), plot.title = element_text(size=22))
 

```

## Application to UK Biobank imaging cohort
```{r fig-ukb-results-health}
var_list = c('20-Smoking Status', '21-BMI Bucket', '23-Diabetes Ever')

plot_data = melt(weight_summary[var %in% var_list & !is.na(bart_prop), .(var, level, pop_prop_weighted, samp_prop, rake_prop, strat_prop, calib_prop, lasso_prop, logit_prop, bart_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]
plot_data[, method := gsub('_prop','', variable)]

plot_data = plot_data[!is.na(value)]

ggplot() +
  geom_bar(data = plot_data[variable %in% c('samp_prop', 'bart_prop'),], aes(x = level, y = as.numeric(as.character(value)), fill = variable), position = 'dodge', stat = 'identity') +
  geom_bar(data = plot_data[variable %in% c('pop_prop_weighted'), ]
           , aes(x = level, y = as.numeric(as.character(value)), color = 'population'), alpha = 0, stat = 'identity') +
   facet_wrap(~var_display, nrow = 2, ncol = 2, scales = 'free_x') + 
  theme_minimal() +
  scale_color_manual(values = c('black', 'red'), name = c(""), labels = c("UK pop", 'sample')) +
  scale_fill_manual(values = c('#9bcbeb', '#418FDE'), name = c("UKB imaging"), labels = c("raw", "weighted")) +
  ggtitle("Weighted health outcome distributions") +
    ylab('') +
    xlab('variable') +
  scale_y_continuous(labels=scales::percent) +
    theme(axis.text.x = element_text(angle = 30, hjust = 1), plot.title = element_text(size=22))
 

```


## Application to the UK Biobank imaging cohort

\begin{figure}
\centering
\includegraphics[width=\textwidth]{~/github/mini-project-1/simulation/results/sim_1_5000_old_v3/plots/ukb_brainvol_age.png}
\end{figure}



## Summary

* Be wary of collider bias when estimating associations in the UK Biobank!

* SCMs are a good framework for thinking about dependence structures, potential sources of bias, and if that bias can be corrected

* **inverse probability weighting** can be used to adjust for selection bias, many different ways to estimate weights

* Weighting the UKB imaging cohort corrects most of the bias in the demographic covariates, but only some of the bias in health outcomes


# Appendix

## Method 1: Post-stratification

Adjust to the *joint* distribution of $\mathbf{Z}$.  This is exactly the definition for recovery (i.e. a sum over all combinations of levels of $\mathbf{Z}$)

1. Define strata based on the full joint distribution of $\mathbf{Z}$
2. Calculate the probability of selection for each stratum
3. Apply stratum-level estimates to individuals

Example: 

\begin{table}[H]
\small
	\centering
	\begin{tabular}{|c c | c c |c | c|}
	\hline
	\textbf{sex} & \textbf{age} & \textbf{N (sample)} & \textbf{N (pop)} & $\hat{\text{P}}(S = 1 | \mathbf{Z}$) & $w^s$\\
	\hline
	Male & under 50 & 35 & 320 & $\frac{35}{320} = 0.109$ & $\frac{0.1}{0.109} = 0.917$\\
	Male & 50 plus & 11 & 133 & 0.083 & 1.20\\
	Female & under 50 & 41 & 355 & 0.115 & 0.870\\
	Female & 50 plus & 13 & 192 & 0.068 & 1.47\\
	\hline
	\end{tabular}
\end{table}

where $\text{P}(S = 1) = 0.1$

Then, all men under 50 in the study are given $w^s = 0.917$


## Method 1: Post-stratification

**Pros**:

* quick, closed-form solution
* weighted joint distribution of $\mathbf{Z}$ exactly matches that of the population

**Cons**:

* $\mathbf{Z}$ must be discrete
* can only consider a limited number of $\mathbf{Z}$ before the strata get too small


## Method 2: Raking

*Iterative proportional fitting*; iteratively adjust the *marginal* distributions of auxilary variables $\mathbf{Z}$

1. Post-stratify to the population sex distribution

2. Post-stratify the *weighted* sample to the population age distribution and update the weights

3. Post-stratify the *new weighted* sample to the population sex distribution and update weights

...

Stop when weights stabilize (according to a tolerance threshold $\epsilon$)


## Method 2: Raking

**Pros**:

* Weights are more stable, less extreme than post-stratification
* Can consider a large set of variables $\mathbf{Z}$

**Cons**:

* Iterative (may never converge)
* Not considering interactions between variables

(Basically the opposite of post-stratification)


## Method 3: Calibration

A generalization of raking that allows for continuous $\mathbf{Z}$

Instead of iterating over marginal distributions of elements in $\mathbf{Z}$ (i.e. P(sex) and P(age)), we iterate over the totals of each level of each element in $\mathbf{Z}$ (i.e. female, male, under 50, over 50).

With this formlation, we can also enforce constraints (weight) on **continuous** variables.
* e.g. we constrain the mean age of the sample

*Con:* can be very finicky, even less likely to converge than raking


## Method 4: Directly estimate $\hat{\text{P}}(S = 1| \mathbf{Z})$ with regression

We use logistic regression to estimate $\hat{\text{P}}(S_i = 1 | \mathbf{Z}_i)$ because selection is binary (S = 1 if observed, 0 otherwise):

$$\hat{\text{P}}(S_i = 1 | \mathbf{Z}_i) = \text{logit}^{-1}(\boldsymbol{\beta}\mathbf{Z}_i)$$

**Pros:**

* Can account for a large $\mathbf{Z}$, including continous variables and interactions
* Don't need custom weighting tools, just logistic regression

**Cons:** 

* Weighted distribution of $\mathbf{Z}$ will almost certainly not match population distributions (making results much less interpretable)
* Requires individual-level population data


## Method 5: Raking with LASSO variable selection

**Problem:** Both raking and post-stratification can fail when $\mathbf{Z}$ is too large

**Solution:** Select significant subsets of $\mathbf{Z}$ using LASSO, then rake to those marginals [@Caughey2017]

General procedure:

1. Specify all levels of $\mathbf{Z}$ and subsets to consider (i.e. all first-order terms, and maybe two-way interactions)
2. Fit LASSO to $S_i = \text{logit}^{-1}(\boldsymbol{\beta}\mathbf{Z}_i)$
3. Fit LASSO to $Y_i = f(\boldsymbol{\beta}\mathbf{Z}_i)$ ($f$ depends on likelihood of $Y$)
4. Rake to marginal distributions of all levels of $\mathbf{Z}$ for which the corresponding $\beta \neq 0$ in either of the LASSOs

**Caution:**  Highly dependent on LASSO performance


## Method 6: BART + raking

**Intuition:** Use **Bayesian additive regression tree (BART)** to estimate $P(S = 1 | \mathbf{Z})$, then rake to selected $\mathbf{Z}$ so that marginal distributions match (for interpretability)

Why a BART?

* Trees are great for interactions
* Some parallels to post-stratification

(but could use any method)

**Caution:** Computation time much greater than other methods


## References {.allowframebreaks} 
\small