---
title: "Methods for Selection Bias in the UK Biobank"
author: "Valerie Bradley"
date: "15 October 2019"
output: 
  beamer_presentation:
    theme: "Madrid"
    slide_level: 2
    includes:
      in_header: cust_formatting.tex
    keep_tex: yes
    toc: true
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(ggplot2)
library(data.table)
library(knitr)
library(kableExtra)
library(gridExtra)
library(grid)
library(lattice)
library(ggpubr)


# plotting function
getDistPlot = function(data, title = NULL){
  
  plot = ggplot() +
  geom_bar(data = data[variable %in% c('samp_prop', 'ukb_prop'),]
           , aes(x = level, y = as.numeric(as.character(value)), fill = variable), position = 'dodge', stat = 'identity') +
  # geom_bar(data = data[variable == ,]
  #          , aes(x = level, y = as.numeric(as.character(value)), color = 'UK pop'), alpha = 0, stat = 'identity') +
      geom_bar(data = data[variable == 'pop_prop_weighted', ]
           , aes(x = level, y = as.numeric(as.character(value)), color = 'population'), alpha = 0, stat = 'identity') +
  facet_wrap(~var_display, nrow = 2, ncol = 2, scales = 'free_x') + 
  theme_minimal() +
  scale_color_manual(values = c('black'), name = c(""), labels = c("UK pop")) +
  scale_fill_manual(values = c('#9bcbeb', '#418FDE'), name = c("UK Biobank"), labels = c("full", "imaging")) +
  #ggtitle(paste0("Distribution comparison - ", var)) +
    ylab('') +
    xlab('variable') +
  scale_y_continuous(labels=scales::percent) +
    theme(axis.text.x = element_text(angle = 30, hjust = 1), plot.title = element_text(size=22))
  
  if(is.null(title)){
    plot = plot + ggtitle("Distributions of variables")
  }else{
    plot = plot + ggtitle(title)
  }
  
  return(plot)
}

load('~/github/mini-project-1/weighting/ukb_weighted_hse16_summary.rda')
# pop_prop = HSE, unweighted
# pop_prop_weighted = HSE, weighted

# ADD IN APOE PREVALENCE
weight_summary[var == '16-ApoE Phenotype', var := '16-ApoE Genotype']


```



# The UK Biobank

## Overview

__UK Biobank__

* Largest ever prospective health study
* Includes genetic sequencing, blood tests, physical exams, health history questionnaire
* 500,000+ participants aged 40-70 when recruited between 2006 and 2010
* Not representative of the UK general population [@fry2017comparison]

__Goal__: Study exposures and outcomes that affect aging populations

__UK Biobank imaging cohort__

* Subset of UKB participants recruited to undergo additional imaging exams
* 21,407 complete, valid T1 structural MRIs



## Unrepresentativeness of the UK Biobank

```{r plot-selection-bias-demos, warning=FALSE, message=FALSE, echo=F, out.width='95%', fig.align='center'}
# head(weight_summary)
# weight_summary[, unique(var)]
# melt(weight_summary[, .(var, level, pop_prop_weighted, samp_prop)],id.vars = c('var', 'level'))
var_list = c('03-Age Bucket', '05-Ethnicity', '15-Home ownership', '10-Occupation')

plot_data = melt(weight_summary[var %in% var_list, .(var, level, pop_prop_weighted, ukb_prop, samp_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]

getDistPlot(plot_data, title = "Demographic covariates")

```
<sub>UK population defined using the [2016 Health Survey for England](https://digital.nhs.uk/data-and-information/publications/statistical/health-survey-for-england/health-survey-for-england-2016)</sub>



## Unrepresentativeness of the UK Biobank

```{r plot-selection-bias-health, warning=FALSE, message=FALSE, echo=F, out.width = '95%', fig.align='center'}

var_list = c('20-Smoking Status', '21-BMI Bucket', '23-Diabetes Ever', '16-ApoE Genotype')

plot_data = melt(weight_summary[var %in% var_list, .(var, level, pop_prop_weighted, ukb_prop, samp_prop)], id.vars = c('var','level'))
plot_data[, var_display := gsub('^[0-9][0-9][-]','',var)]

getDistPlot(data = plot_data, title = "Health covariates")
```

# Why is unrepresentativeness a problem?

## Quick note on structural causal models (SCMs)

SCMs use directed acyclic graphs (DAGs) as ``a mathematical language for integrating statistical and subject-matter information," specifically information about dependence structures [@Pearl1995]

* __nodes__ represent *physical mechanisms*
* __edges__ represent *direct causal pathways*

\begin{figure}
\centering
\begin{tikzpicture}
  	\node[state, minimum width=0.5cm] (w) at (-2,0) {W};
	\node[state, minimum width=0.5cm] (x) at (0,0) {X};
	\node[state, minimum width=0.5cm] (y) at (2,0) {Y};
	\node[state, minimum width=0.5cm] (z) at (0,1) {Z};
	
		\path (x) edge (y);
	\path (w) edge (x);
	\path (z) edge (y);	
\end{tikzpicture}
\end{figure}

**Note**: we will focus on discrete mechanisms (e.g. \(X \in \{0,1\}\)), but this can be generalized to the continuous case


## SCM Example

\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES};
	\node[state, double] (s) at (2.5,0) {Selection};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol.};
	\node[state] (z) at (5,1.25) {Dementia};

	\path (x) edge[green] (s);
	\path (y) edge (z);
	\path (z) edge[red] (s);
	%\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

* SES and participation in UKB are \textcolor{green}{positively} correlated: Lower SES may mean it's harder to take time off of work, find childcare, etc.
* Dementia and participation are \textcolor{red}{negatively} correlated: Individuals with dementia (even early signs) may find it harder to make it to an appointment
* SES and hippocampal volume are independent (no edge)

What can we infer about someone with low SES who has participated in the UKB?

## Collider bias

```{r fig-2, out.width='40%'}
# include_graphics('~/github/mini-project-1/presentation/selection-bias-ex'
#                  , auto_pdf = getOption("knitr.graphics.auto_pdf", FALSE)
#                  , dpi = NULL)
```
\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES (E)};
	\node[state, double] (s) at (2.5,0) {Selection (S)};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol. (H)};
	\node[state] (z) at (5,1.25) {Dementia (D)};

	\path (x) edge (s);
	\path (y) edge (z);
	\path (z) edge (s);
	\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

**Collider bias**: "When two variables independently influence a third variable, and that third variable is conditioned upon" [@Munafo2018]

Opens a *backdoor path* (spurious association) between SES (E) and hippocampal volume (H).

* Want to know \(\text{P(H|E)}\)
* But only observe \(\text{P}(H | E, S = 1)\)

## Collider bias example

@Day2016: GWAS using 142,630 observations from the UKB, induced strong collider bias

\begin{figure}
\centering
\includegraphics[width=\textwidth]{collider_bias_example.png}
\end{figure}


# Can we recover from selection bias?


## Formal conditions for recovery

Consider the *association* between \(X\) and \(Y\), or \(\text{P}(Y = y|X = x)\)

We can recover from selection bias **if and only if** \(\text{P}(y|x)\) can be written in terms of the quantities observed under selection, usually relying on a set of *auxilary variables* \(\mathbf{Z}\)

* if \(Y \ci S | {\mathbf{Z}, X}\), then we can recover the association with
$$\text{P}(y|x) = \sum_\mathbf{z} \text{P}(y|x, \mathbf{z}, S=1)\text{P}(\mathbf{z}|x)$$
* If \(\{Y \cup X\} \ci S | \mathbf{Z}\), then we can recover the association with
$$\text{P}(y|x) = \sum_\mathbf{z} \text{P}(y|x, \mathbf{z}, S=1)\text{P}(\mathbf{z})$$

The key is that conditioning on \(\mathbf{Z}\) (and maybe \(X\)) makes the outcome of interest \(Y\) *conditionally independent of selection* \(S\)

## Example

Back to our example...

\begin{figure}
\centering
\begin{tikzpicture}
	\node[state] (x) at (0,1.25) {SES (E)};
	\node[state, double] (s) at (2.5,0) {Selection (S)};
	
	\node[state] (y) at (2.5,2.5) {Hippoc. Vol. (H)};
	\node[state] (z) at (5,1.25) {Dementia (D)};

	\path (x) edge (s);
	\path (y) edge (z);
	\path (z) edge (s);
	\path [dashed, >=latex, style={<->}, fill=oxfordred, color=oxfordred] (x) edge [bend left=10]  (z);
	
	\end{tikzpicture}
\end{figure}

True that \(H \ci S | D, E\), but not true that \(\{H \cup E\} \ci S | D\)

Therefore, we CAN recover \(\text{P}(H | E)\) as
$$\text{P}(H | E) = \sum_D\text{P}(H | E, D, S = 1) P (D|E)$$
**only if** we observe \(\text{P}(D | E)\) (or have an unbiased estimate)

## Problems

1. Must assume we have correctly represented the dependence structures (and have correctly identified all necessary elements of $\mathbf{Z}$).  This is __hard__ to do in practice.

2. Don't always observe \(\text{P}(\mathbf{Z} | X)\) or \(\text{P}(\mathbf{Z})\) in full.  *Can we estimate it?*

3. Can only condition on a limited number of discrete variables $\mathbf{Z}$ before this breaks down.  *What happens when an element of $\mathbf{Z}$ is continuous?*


# Methods for recovering from selection bias

## Two main methods for recovery
Classes of methods for estimating effects, associations, prevalence in the presence of selection bias:

* **Inverse probability weighting (IPW)**: weight each observed unit by the inverse of their probability of selection; intuitively, creates a ``pseudo-population" in which selection bias does not exist [@Hernan2004]
  * con: not always clear how to incorporate into estimators
  * con: hard to correctly estimate standard errors of weighted estimators

* **Regression methods**: directly model the outcome of interest, accounting for confounders/auxiliary variables 
  * con: have to estimate separate regression model for each outcome



## Inverse probability weighting (IPW)

Can be derived exactly from (causal) recovery conditions [@Correa2018]:
\begin{align}
\text{P}(\mathbf{y}|do(\mathbf{x})) &= \sum_\mathbf{Z} \text{P}(\mathbf{y}|\mathbf{x}, \mathbf{z}, S=1)\text{P}(\mathbf{z}\setminus\mathbf{z^T}|\mathbf{z^T}, S = 1)\text{P}(\mathbf{z^T})\\
&= \sum_\mathbf{Z} \frac{\textcolor{oxfordred}{\text{P}(\mathbf{y}, \mathbf{x}, \mathbf{z} | S = 1)}}{\textcolor{oxfordlightblue}{\text{P}(\mathbf{x}| \mathbf{z}, S = 1)}} \textcolor{oxfordgreen}{\frac{\text{P}(S = 1)}{\text{P}(S = 1 | \mathbf{z^T})}} \label{eq:IPSW}
\end{align}
\begin{itemize}
	\item  $\textcolor{oxfordred}{\text{P}(\mathbf{y}, \mathbf{x}, \mathbf{z} | S = 1)}$, joint distribution of $\mathbf{Y}$, $\mathbf{X}$ and $\mathbf{Z}$ under selection bias
	\item $\textcolor{oxfordlightblue}{\text{P}(\mathbf{x}| \mathbf{z}, S = 1)}$, the probability of treatment given covariates in the selection-biased sample data, related to the *propensity score* $\text{P}(X|\mathbf{Z})$
	\item $\textcolor{oxfordgreen}{\text{P}(S = 1)/\text{P}(S = 1 | \mathbf{z^T})}$, the \textit{inverse probability-of-selection weight}
\end{itemize}


## Inverse probability weighting (IPW)

Say we observe outcomes $Y$, exposures $X$, and auxiliary variables $Z$ for $i = 1, \dots, n$ individuals. Can estimate $\text{P}(Y | X)$ with

$$\hat{\mu} = E(Y | X = x) = \frac{1}{n}\sum_{i = 1}^n \textcolor{oxfordgreen}{w_i^s} \textcolor{oxfordlightblue}{w_i^c} \textcolor{oxfordred}{I_{(X_i = x)}}Y_i$$

* $\textcolor{oxfordgreen}{w_i^{s}}$ is the inverse probability of selection weight, given $\mathbf{z}$
$$w_i^{s} = \hat{\text{P}}(S = 1)/\hat{\text{P}}(S = 1 | \mathbf{Z}_i)$$
* $\textcolor{oxfordlightblue}{w_i^c}$ is the probability of treatment under selection
$$w_i^c = 1/\hat{\text{P}}(X_i|\mathbf{Z}_i, S = 1)$$
* $\textcolor{oxfordred}{I_{(X_i = x)}}$ is an indicator function for which exposure a unit recieved

**But how do we estimate the weights?**

## Methods: summary

Classic weighting methods:

1. *Post-stratification*: adjust to the joint distribution of \(\mathbf{Z}\) 
2. *Raking*: iteratively adjust the marginal distributions of elements in \(\mathbf{Z}\)
3. *Calibration*: raking, but with continuous variables as well as discrete variables

Less-common methods:

4. *Logit*: estimate the probability of selection directly
5. *LASSO*: use a LASSO to select variables and interactions for raking

New method:

6. *BART + raking*: use a Bayesian Additive Regression Tree (BART) to estimate the probability of selection, then rake such that key marginal distributions match those of the population


## Methods: Post-stratification

Adjust to the *joint* distribution of $\mathbf{Z}$.  This is exactly the definition for recovery (i.e. a sum over all combinations of levels of $\mathbf{Z}$)

1. Define strata based on the full joint distribution of $\mathbf{Z}$
2. Calculate the probability of selection for each stratum
3. Apply stratum-level estimates to individuals

Example: 

\begin{table}[H]
\small
	\centering
	\begin{tabular}{|c c | c c |c | c|}
	\hline
	\textbf{sex} & \textbf{age} & \textbf{N (sample)} & \textbf{N (pop)} & $\hat{\text{P}}(S = 1 | \mathbf{Z}$) & $w^s$\\
	\hline
	Male & under 50 & 35 & 320 & $\frac{35}{320} = 0.109$ & $\frac{0.1}{0.109} = 0.917$\\
	Male & 50 plus & 11 & 133 & 0.083 & 1.20\\
	Female & under 50 & 41 & 355 & 0.115 & 0.870\\
	Female & 50 plus & 13 & 192 & 0.068 & 1.47\\
	\hline
	\end{tabular}
\end{table}

where $\text{P}(S = 1) = 0.1$

Then, all men under 50 in the study are given $w^s = 0.917$


## Methods: Post-stratification

**Pros**:

* quick, closed-form solution
* weighted joint distribution of $\mathbf{Z}$ exactly matches that of the population

**Cons**:

* $\mathbf{Z}$ must be discrete
* can only consider a limited number of $\mathbf{Z}$ before the strata get too small


## Methods: Raking

*Iterative proportional fitting*; iteratively adjust the *marginal* distributions of auxilary variables $\mathbf{Z}$

1. Post-stratify to the population sex distribution

2. Post-stratify the *weighted* sample to the population age distribution and update the weights

3. Post-stratify the *new weighted* sample to the population sex distribution and update weights

...

Stop when weights stabilize (according to a tolerance threshold $\epsilon$)


## Methods: Raking

**Pros**:

* Weights are more stable, less extreme than post-stratification
* Can consider a large set of variables $\mathbf{Z}$

**Cons**:

* Iterative (may never converge)
* Not considering interactions between variables

(Basically the opposite of post-stratification)


## Methods: Calibration

A generalization of raking that allows for continuous $\mathbf{Z}$

Instead of iterating over marginal distributions of elements in $\mathbf{Z}$ (i.e. P(sex) and P(age)), we iterate over the totals of each level of each element in $\mathbf{Z}$ (i.e. female, male, under 50, over 50).

With this formlation, we can also enforce constraints (weight) on **continuous** variables.
* e.g. we constrain the mean age of the sample

*Con:* can be very finicky, even less likely to converge than raking


## Methods: Directly estimate $\hat{\text{P}}(S = 1| \mathbf{Z})$ with regression

We use logistic regression to estimate $\hat{\text{P}}(S_i = 1 | \mathbf{Z}_i)$ because selection is binary (S = 1 if observed, 0 otherwise):

$$\hat{\text{P}}(S_i = 1 | \mathbf{Z}_i) = \text{logit}^{-1}(\boldsymbol{\beta}\mathbf{Z}_i)$$

**Pros:**

* Can account for a large $\mathbf{Z}$, including continous variables and interactions
* Don't need custom weighting tools, just logistic regression

**Cons:** 

* Weighted distribution of $\mathbf{Z}$ will almost certainly not match population distributions (making results much less interpretable)
* Requires individual-level population data


## Methods: Raking with LASSO variable selection

**Problem:** Both raking and post-stratification can fail when $\mathbf{Z}$ is too large

**Solution:** Select significant subsets of $\mathbf{Z}$ using LASSO

# Application to the UK Biobank

## Simulation Results
\center
```{r fig-sim-results, out.width='100%'}
include_graphics('~/github/mini-project-1/simulation/results/sim_1_5000_old_v3/plots/mse_by_sample_size.png'
                 , auto_pdf = getOption("knitr.graphics.auto_pdf", FALSE)
                 , dpi = NULL)
```


## Application to UK Biobank
```{r fig-ukb-results}
ggplot(weight_summary[level %in% c('Female', '01-White', '1', '01-Under 18k', '01-College plus/profesh', '01-Own outright','02-Own with mortgage') | var %in% c('03-Age Bucket', '11-Income')]) +
  geom_point(aes(x = as.numeric(as.character(pop_prop_weighted)), y = paste0(var, ': ',level), color = 'Population')) +
  geom_point(aes(x = as.numeric(as.character(samp_prop)), y = paste0(var, ': ',level), color = 'UKB Imaging cohort')) +
geom_point(aes(x = as.numeric(as.character(bart_prop)), y = paste0(var, ': ',level), color = 'BART adjusted')) + 
  ggtitle("UK Population v. UKB Imaging cohort") +
  xlab("proportion of group") +
  ylab("")
```




## Summary

* The UKB and the imaging cohort are not representative of the UK general population
* This is a problem because **collider bias** can impact estimates of association
* Reviewed SCMs as a language for expressing dependence structures


## References {.allowframebreaks} 
\small